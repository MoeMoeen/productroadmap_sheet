#!/usr/bin/env python3
"""
End-to-end test for multi-model scoring flow.

Tests the complete flow:
1. Create isolated test initiative with multiple math models (each targeting different KPIs)
2. Run scoring via score_initiative_all_frameworks (simulates pm.score_selected workflow)
3. Verify individual model.computed_score values match expected formula calculations
4. Verify KPI contributions are correctly aggregated by target_kpi_key
5. Verify PM override protection works
6. Test edge cases: multiple models per KPI, invalid KPI keys, no primary model
"""

import sys
from datetime import datetime, timezone
from pathlib import Path

# Add project root to path FIRST (required for app imports)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# App imports MUST come after sys.path modification
from app.db.models.initiative import Initiative  # noqa: E402
from app.db.models.optimization import OrganizationMetricConfig  # noqa: E402
from app.db.models.scoring import InitiativeMathModel  # noqa: E402
from app.db.session import SessionLocal  # noqa: E402
from app.services.product_ops.kpi_contribution_adapter import compute_kpi_contributions  # noqa: E402
from app.services.product_ops.scoring_service import ScoringService  # noqa: E402


def setup_test_data(db):
    """Create ISOLATED test initiative with multiple math models."""
    
    # 1. Ensure test KPIs exist in OrganizationMetricConfig
    kpis = [
        ("revenue", "Revenue", "north_star", "USD"),
        ("user_retention", "User Retention", "strategic", "%"),
        ("engagement_score", "Engagement Score", "strategic", "points"),
        ("invalid_kpi", "Invalid KPI", "operational", "units"),  # Will be filtered out
    ]
    
    for kpi_key, kpi_name, kpi_level, unit in kpis:
        existing = db.query(OrganizationMetricConfig).filter_by(kpi_key=kpi_key).first()
        if not existing:
            db.add(OrganizationMetricConfig(
                kpi_key=kpi_key,
                kpi_name=kpi_name,
                kpi_level=kpi_level,
                unit=unit,
            ))
    
    db.commit()
    
    # 2. Create CLEAN test initiative (not reusing production data!)
    # Use unique key with timestamp to avoid conflicts
    test_key = f"TEST-MULTI-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}"
    
    initiative = Initiative(
        initiative_key=test_key,
        title="Test Multi-Model Scoring Initiative",
        department="Engineering",
        country="US",
        active_scoring_framework="MATH_MODEL",
        use_math_model=True,
        problem_statement="Isolated test for multi-model scoring with KPI contributions",
    )
    
    db.add(initiative)
    db.flush()
    
    # 3. Create math models with KNOWN formulas for validation
    models = [
        # Model 1: Revenue (primary) - Expected: 1000 * 0.05 * 100 = 5000.0
        InitiativeMathModel(
            initiative_id=initiative.id,
            model_name="Revenue Impact Model",
            target_kpi_key="revenue",
            formula_text="traffic = 1000\nconversion_rate = 0.05\naverage_order_value = 100\nvalue = traffic * conversion_rate * average_order_value\neffort = 30",
            metric_chain_text="traffic ‚Üí conversion ‚Üí revenue",
            is_primary=True,
            approved_by_user=True,
            framework="MATH_MODEL",
        ),
        # Model 2: Retention - Expected: 10000 * 0.02 * 100 = 20000.0
        InitiativeMathModel(
            initiative_id=initiative.id,
            model_name="Retention Impact Model",
            target_kpi_key="user_retention",
            formula_text="active_users = 10000\nchurn_reduction = 0.02\nvalue = active_users * churn_reduction * 100\neffort = 20",
            metric_chain_text="feature_usage ‚Üí satisfaction ‚Üí retention",
            is_primary=False,
            approved_by_user=True,
            framework="MATH_MODEL",
        ),
        # Model 3: Engagement - Expected: 5000 * 0.15 * 10 = 7500.0
        InitiativeMathModel(
            initiative_id=initiative.id,
            model_name="Engagement Impact Model",
            target_kpi_key="engagement_score",
            formula_text="users_impacted = 5000\nengagement_uplift = 0.15\nvalue = users_impacted * engagement_uplift * 10\neffort = 15",
            metric_chain_text="feature_adoption ‚Üí usage_frequency ‚Üí engagement",
            is_primary=False,
            approved_by_user=True,
            framework="MATH_MODEL",
        ),
        # Model 4: Revenue (duplicate KPI) - Expected: 2000 * 0.03 * 50 = 3000.0
        # This tests multiple models targeting SAME KPI
        InitiativeMathModel(
            initiative_id=initiative.id,
            model_name="Revenue Alt Model",
            target_kpi_key="revenue",
            formula_text="traffic = 2000\nconversion_rate = 0.03\naverage_order_value = 50\nvalue = traffic * conversion_rate * average_order_value\neffort = 25",
            metric_chain_text="alt_traffic ‚Üí alt_conversion ‚Üí revenue",
            is_primary=False,
            approved_by_user=True,
            framework="MATH_MODEL",
        ),
        # Model 5: Invalid KPI (operational level, should be filtered)
        InitiativeMathModel(
            initiative_id=initiative.id,
            model_name="Invalid KPI Model",
            target_kpi_key="invalid_kpi",
            formula_text="value = 999\neffort = 10",
            metric_chain_text="should ‚Üí be ‚Üí filtered",
            is_primary=False,
            approved_by_user=True,
            framework="MATH_MODEL",
        ),
    ]
    
    for model in models:
        db.add(model)
    
    db.commit()
    db.refresh(initiative)
    
    print(f"‚úÖ Created test initiative {initiative.initiative_key} with {len(models)} math models")
    return initiative


def test_multi_model_scoring(db):
    """Test complete multi-model scoring flow with PROPER validation."""
    
    print("\n" + "="*80)
    print("TEST: Multi-Model Scoring End-to-End")
    print("="*80)
    
    # Setup
    initiative = setup_test_data(db)
    
    # Step 1: Verify models were created
    print(f"\n1Ô∏è‚É£  Verifying {len(initiative.math_models)} models created...")
    assert len(initiative.math_models) == 5, f"Expected 5 models, got {len(initiative.math_models)}"
    
    models_by_name = {m.model_name: m for m in initiative.math_models}
    for name in ["Revenue Impact Model", "Retention Impact Model", "Engagement Impact Model", "Revenue Alt Model", "Invalid KPI Model"]:
        assert name in models_by_name, f"Model {name} not found"
        print(f"   ‚úì {name} (target: {models_by_name[name].target_kpi_key}, primary: {models_by_name[name].is_primary})")
    
    # Step 2: Run scoring (simulates pm.score_selected workflow)
    print("\n2Ô∏è‚É£  Running scoring service (score_initiative_all_frameworks)...")
    svc = ScoringService(db)
    svc.score_initiative_all_frameworks(initiative, enable_history=False)
    db.commit()
    db.refresh(initiative)
    
    # Step 3: Verify initiative-level scores populated
    print("\n3Ô∏è‚É£  Verifying initiative-level math scores...")
    print(f"   - math_value_score: {initiative.math_value_score}")
    print(f"   - math_effort_score: {initiative.math_effort_score}")
    print(f"   - math_overall_score: {initiative.math_overall_score}")
    
    assert initiative.math_value_score is not None, "math_value_score should be populated"
    assert float(initiative.math_value_score) == 5000.0, f"math_value_score should be 5000.0 (primary model), got {initiative.math_value_score}"  # type: ignore[arg-type]
    assert float(initiative.math_effort_score) == 30.0, f"math_effort_score should be 30.0, got {initiative.math_effort_score}"  # type: ignore[arg-type]
    print("   ‚úì Initiative scores match primary model (5000.0 value, 30.0 effort)")
    
    # Step 4: Verify individual model.computed_score matches EXPECTED formula calculations
    print("\n4Ô∏è‚É£  Verifying individual model.computed_score values match formulas...")
    
    expected_scores = {
        "Revenue Impact Model": 5000.0,   # 1000 * 0.05 * 100
        "Retention Impact Model": 20000.0, # 10000 * 0.02 * 100
        "Engagement Impact Model": 7500.0, # 5000 * 0.15 * 10
        "Revenue Alt Model": 3000.0,       # 2000 * 0.03 * 50
        "Invalid KPI Model": 999.0,        # value = 999
    }
    
    for model in initiative.math_models:
        expected = expected_scores[model.model_name]
        actual = model.computed_score
        
        assert actual is not None, f"Model {model.model_name} computed_score is None"
        assert abs(actual - expected) < 0.01, f"Model {model.model_name}: expected {expected}, got {actual}"
        print(f"   ‚úì {model.model_name}: {actual} (expected: {expected})")
    
    # Step 5: Verify KPI contributions aggregated correctly
    print("\n5Ô∏è‚É£  Verifying KPI contributions aggregation...")
    
    # Manually compute what we expect
    computed = compute_kpi_contributions(initiative)
    print(f"   - Computed contributions: {computed}")
    
    # Revenue has 2 models:
    # - Revenue Impact Model (5000) - PRIMARY
    # - Revenue Alt Model (3000)
    # Logic: Primary wins, so revenue = 5000 (NOT summed)
    assert "revenue" in computed, "Revenue contribution missing"
    assert computed["revenue"] == 5000.0, f"Revenue should be 5000 (primary model), got {computed['revenue']}"
    print(f"   ‚úì Revenue from primary model: {computed['revenue']} (alt model ignored)")
    
    assert "user_retention" in computed, "User retention contribution missing"
    assert computed["user_retention"] == 20000.0, f"Expected 20000, got {computed['user_retention']}"
    print(f"   ‚úì User retention: {computed['user_retention']}")
    
    assert "engagement_score" in computed, "Engagement contribution missing"
    assert computed["engagement_score"] == 7500.0, f"Expected 7500, got {computed['engagement_score']}"
    print(f"   ‚úì Engagement score: {computed['engagement_score']}")
    
    # Invalid KPI should be FILTERED OUT (operational level not allowed)
    # NOTE: This happens in validate_kpi_keys() called by update_initiative_contributions()
    # but compute_kpi_contributions() returns all keys
    if "invalid_kpi" in computed:
        print(f"   ‚ö†Ô∏è  Invalid KPI present in raw computation: {computed['invalid_kpi']} (will be filtered)")
    
    # Verify DB columns match (after validation filters invalid keys)
    assert initiative.kpi_contribution_computed_json is not None
    assert "revenue" in initiative.kpi_contribution_computed_json
    assert "invalid_kpi" not in initiative.kpi_contribution_computed_json, "Invalid KPI should be filtered in DB"
    print("   ‚úì Invalid KPI (operational level) correctly filtered from DB columns")
    
    assert str(initiative.kpi_contribution_source) == "computed"
    print("   ‚úì DB columns populated correctly (source=computed)")
    
    # Step 6: Test PM override protection
    print("\n6Ô∏è‚É£  Testing PM override protection...")
    
    # PM manually overrides contributions
    override_values = {"revenue": 999.99, "custom_kpi": 123.45}
    initiative.kpi_contribution_json = override_values  # type: ignore[assignment]
    initiative.kpi_contribution_source = "pm_override"  # type: ignore[assignment]
    db.commit()
    db.refresh(initiative)
    
    print(f"   - PM set override: {override_values}")
    
    # Re-run scoring - should update computed but NOT overwrite pm_override
    print("   - Re-running scoring...")
    svc.score_initiative_all_frameworks(initiative, enable_history=False)
    db.commit()
    db.refresh(initiative)
    
    # Verify computed_json updated but active json preserved
    print(f"   - Expected computed: {computed}")
    print(f"   - Actual computed_json: {initiative.kpi_contribution_computed_json}")
    print(f"   - Active json (should preserve override): {initiative.kpi_contribution_json}")
    
    # The computed dict includes invalid_kpi, but DB should filter it
    expected_computed = {k: v for k, v in computed.items() if k != 'invalid_kpi'}
    actual_computed_json = dict(initiative.kpi_contribution_computed_json or {})  # type: ignore[arg-type]
    actual_active_json = dict(initiative.kpi_contribution_json or {})  # type: ignore[arg-type]
    actual_source = str(initiative.kpi_contribution_source)
    
    assert actual_computed_json == expected_computed, f"Computed JSON should be updated. Expected {expected_computed}, got {actual_computed_json}"
    assert actual_active_json == override_values, "Active JSON should preserve PM override"
    assert actual_source == "pm_override", "Source should remain pm_override"
    print(f"   ‚úì PM override protection working: computed={expected_computed}, active={override_values}")
    
    # Step 7: Verify primary model selection
    print("\n7Ô∏è‚É£  Verifying primary model selection...")
    primary_models = [m for m in initiative.math_models if m.is_primary]
    assert len(primary_models) == 1, f"Expected 1 primary model, got {len(primary_models)}"
    
    primary = primary_models[0]
    assert str(primary.model_name) == "Revenue Impact Model", "Expected Revenue Impact Model as primary"
    assert float(initiative.math_value_score) == float(primary.computed_score), "Initiative score should match primary model"  # type: ignore[arg-type]
    print(f"   ‚úì Primary model: {primary.model_name}, score: {primary.computed_score}")
    
    # Step 8: Verify provenance tracking
    print("\n8Ô∏è‚É£  Verifying provenance tracking...")
    assert initiative.scoring_updated_source is not None, "scoring_updated_source should be set"
    assert initiative.scoring_updated_at is not None, "scoring_updated_at should be set"
    print(f"   ‚úì Provenance: source={initiative.scoring_updated_source}, at={initiative.scoring_updated_at}")
    
    print("\n" + "="*80)
    print("‚úÖ ALL TESTS PASSED WITH PROPER VALIDATION!")
    print("="*80)
    print("\nValidated:")
    print("  ‚úì Multiple models per KPI (primary model wins, not summed)")
    print("  ‚úì Formula calculations match expected values")
    print("  ‚úì Invalid KPI keys filtered out (operational level)")
    print("  ‚úì KPI contribution selection logic (primary > max score)")
    print("  ‚úì PM override protection preserves manual edits")
    print("  ‚úì Primary model selection affects initiative-level score")
    print("  ‚úì Provenance tracking records scoring actions")
    
    return initiative


def cleanup_test_data(db, initiative):
    """Properly clean up test data - delete initiative and cascade models."""
    if initiative:
        # Delete the entire test initiative (models cascade delete)
        db.delete(initiative)
        db.commit()
        print(f"\nüßπ Cleaned up test initiative {initiative.initiative_key} and all models")


def main():
    """Run end-to-end test with proper cleanup."""
    db = SessionLocal()
    initiative = None
    
    try:
        initiative = test_multi_model_scoring(db)
        
        if not initiative:
            print("\n‚ùå Test setup failed")
            return 1
        
        print("\n‚úÖ Test completed successfully!")
        
    except AssertionError as e:
        print(f"\n‚ùå TEST ASSERTION FAILED: {e}")
        import traceback
        traceback.print_exc()
        return 1
    except Exception as e:
        print(f"\n‚ùå TEST ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        # ALWAYS cleanup test data
        if initiative:
            try:
                cleanup_test_data(db, initiative)
            except Exception as e:
                print(f"‚ö†Ô∏è  Cleanup failed: {e}")
        db.close()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
