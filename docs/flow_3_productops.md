---

## 1Ô∏è‚É£ Big picture: Sheets as the Product Control Plane

Right now:

* Central backlog sheet = **shared source of truth + presentation** for everyone.
* Intake sheets = **front-end** for intake from departments.
* Flow 1 + Flow 2 = **back-end engine**.

What you‚Äôre designing now:

> A **Product Workbook** (one Google spreadsheet) as the *exclusive playground + control panel* for the Product team.

That workbook would have multiple tabs, at least:

1. **Scoring Inputs / Experiments tab**

   * Per initiative + per framework inputs.
   * Product team freely adds/removes/tweaks columns like:

     * `RICE: Reach`
     * `RICE: Impact`
     * `WSJF: Business Value`
     * `WSJF: Time Criticality`
     * `MATH: parameter_1`, `MATH: parameter_2`, etc.
   * Backend:

     * Reads these raw inputs.
     * Maps them into `ScoreInputs` per framework.
     * Computes scores.
     * Writes results back (e.g. ‚ÄúRICE Score‚Äù, ‚ÄúWSJF Score‚Äù, comments).

2. **Config tab**

   * Mirrors / replaces the stuff in `config.py` and `.env` that should be **product-owned** decisions, not dev-only:

     * `SCORING_DEFAULT_FRAMEWORK`
     * Batch sizes / thresholds
     * Allowed statuses
     * Switches like ‚ÄúEnable math model‚Äù, ‚ÄúUse AI suggestions by default‚Äù, etc.
   * Backend:

     * Treats this tab as a **config source of truth**.
     * Reads values on flow start.
     * Persists some to DB as needed.
   * Think of it as: *instead of editing `config.py` and redeploying, PMs tune the system from Sheets.*

3. **Future Simulation / Optimization tabs**

   * Monte Carlo simulations, capacity simulations.
   * Linear / multi-objective optimization (e.g. ‚Äúgiven budget X, maximize total impact score‚Äù).
   * Scenario tabs like:

     * ‚ÄúScenario A ‚Äì focus on EMEA‚Äù
     * ‚ÄúScenario B ‚Äì aggressive infra cut‚Äù
   * Backend:

     * Reads scenario specs and parameters.
     * Runs simulation/optimization.
     * Writes outputs: recommended prioritization, expected value distributions, risk bands, etc.


* **Backlog sheet** = simple, stable, largely read-only for non-product stakeholders + selected control like active framework.
* **Product workbook** = messy, rich, flexible, experimental control panel just for Product.

That‚Äôs a very healthy separation.

---

## 2Ô∏è‚É£ How to structure the ‚ÄúScoring Inputs‚Äù tab

You said:

> ‚Äú‚Ä¶each column is related to at least one or many frameworks. The name of each column should also indicate which framework(s) it belongs to.‚Äù

I‚Äôd shape it like this.

### Option A ‚Äì Wide, namespaced columns per framework

Each row = one initiative (by key). Columns like:

| Initiative Key | RICE: Reach | RICE: Impact | RICE: Confidence | RICE: Effort | WSJF: Business Value | WSJF: Time Criticality | WSJF: Risk Reduction | WSJF: Job Size | Comments |
| -------------- | ----------- | ------------ | ---------------- | ------------ | -------------------- | ---------------------- | -------------------- | -------------- | -------- |

* Column header convention:

  * `<FRAMEWORK>: <parameter>` or `<framework>.<param>`

Backend logic:

* Read header row.
* Parse headers to detect:

  * `framework` (RICE / WSJF / MATH_MODEL / ‚Ä¶)
  * `parameter_name` (`reach`, `impact`, `job_size`, etc.)
* For each row:

  * Look up `initiative_key`.
  * For each framework:

    * Collect all `framework: param` columns into a `ScoreInputs` instance.
    * Call `engine.compute()` for that framework.
    * Persist scores & optional comments.

This lets PMs:

* Add new columns like `RICE: Override Impact` without breaking anything.
* Add a new framework later (`MATH: parameter_1` etc.) and you just extend your header-to-input mapping rules.

### Option B ‚Äì Long form (one row per initiative+framework)

Alternative schema where each row = (initiative, framework):

| Initiative Key | Framework | Param Name | Param Value | Scenario | Comment |
| -------------- | --------- | ---------- | ----------- | -------- | ------- |

This is more flexible but more complex for PMs to manage. For Sheets, **Option A** (wide with namespaced headers) is nicer UX.

I‚Äôd start with Option A.

---

## 3Ô∏è‚É£ How backend would plug this in

Architecturally, similar pattern to intake/backlog:

1. **Config for Product Workbook**

   * Add a new section in config / JSON, e.g.:

     ```json
     {
       "product_workbook": {
         "spreadsheet_id": "....",
         "tabs": {
           "scoring_inputs": "Scoring_Inputs",
           "config": "Config",
           "simulations": "Simulations"
         }
       }
     }
     ```

2. **New reader service** for scoring inputs

   * `app/sheets/scoring_inputs_reader.py`
   * Reads `Scoring_Inputs` tab:

     * Gets header row
     * Builds a mapping: `(framework, param_name) -> column_index`
     * Returns rows as:

       ```python
       List[{
         "initiative_key": "INIT-000001",
         "framework_inputs": {
             "RICE": {...ScoreInputs fields...},
             "WSJF": {...},
         }
       }]
       ```

3. **New job** for scoring from product sheet

   * `run_scoring_from_sheet(db)`:

     * Read scoring inputs from sheet.
     * For each row and framework:

       * Option A: override or complement values on `Initiative` (e.g. store these in `InitiativeScore.inputs_json` only).
       * Option B: treat these as the canonical inputs for scoring and skip Initiative fields.
     * Call `ScoringService` with those `ScoreInputs` (we may extend `score_initiative` to accept explicit `inputs` instead of deriving from Initiative fields).

4. **Back to central backlog**

   * Once scores are persisted to DB, Flow 1 backlog sync writes the final scores + active framework back to central backlog.

So the ‚Äúproduct sheet scoring flow‚Äù is:

> Product sheet (Scoring_Inputs tab) ‚Üí Scoring job ‚Üí DB ‚Üí Backlog sync ‚Üí Central backlog sheet

Central backlog never sees all the messy knobs; just the outputs.

---

## 4Ô∏è‚É£ Config tab ‚Äì replacing hardcoded config with sheet config

You also said:

> ‚Äúwe should reflect all the configs we are manually changing in `config.py` to a config tab‚Ä¶ That config tab is the source of truth.‚Äù

I think that‚Äôs very powerful, *if* we do it selectively.

### Good candidates for sheet-based config

* `SCORING_DEFAULT_FRAMEWORK`
* `SCORING_BATCH_COMMIT_EVERY`
* Thresholds:

  * ‚ÄúMinimum confidence to accept RICE score‚Äù
  * ‚ÄúMaximum effort days for ‚Äòquick wins‚Äô‚Äù
* Toggles:

  * Enable math models
  * Enable AI scoring
  * Whether to use history table or not
* Display-related config for backlog sheet:

  * Which columns to show / hide in view modes
* Per-org / per-team weights and coefficients.

### Less good candidates

* DB connection details
* Service account file paths
* Anything that belongs to infra/devops rather than product behavior.

### How to implement

Config tab schema could be as simple as:

| Key                        | Value | Type   | Scope   | Description                         |
| -------------------------- | ----- | ------ | ------- | ----------------------------------- |
| SCORING_DEFAULT_FRAMEWORK  | RICE  | string | global  | Default framework when none set     |
| SCORING_BATCH_COMMIT_EVERY | 200   | int    | scoring | Batch size for scoring jobs         |
| USE_AI_SUGGESTED_SCORES    | true  | bool   | scoring | Whether to use LLM-suggested scores |

Backend:

* New reader: `ConfigSheetReader` that reads `Config` tab into a dict.
* New small service: `ConfigService` that:

  * Merges:

    * `.env` / `config.py` defaults,
    * DB stored config (optional),
    * sheet-based overrides.
* Then `settings` in the runtime could be a composition: base config + dynamic config loaded at job start.

We don‚Äôt need to jump there right now, but your direction is absolutely sound: **product-behavior config belongs in a product-facing control surface, not in Python code**.

---

## 5Ô∏è‚É£ Simulation / Optimization tabs

This fits perfectly as a *third layer* in the same workbook:

* Tabs like:

  * `Simulations_Roadmap_v1`
  * `Capacity_Scenarios`
  * `MonteCarlo_Impact`

Each defines:

* Inputs: distributions, constraints, budgets, scenario labels.
* Backend runs:

  * Monte Carlo
  * Linear programming / MILP for capacity allocation
  * Multi-objective optimization (value vs risk vs cost)
* Outputs:

  * Recommended subsets of initiatives
  * Expected value & risk distributions
  * Visuals (which we might export as images or just numeric tables)

The pattern doesn‚Äôt change:

> Sheet tab = declarative scenario
> Backend = engine
> Central backlog or other sheets = rendered outcome

---

## üß© Updated Flow 3 ‚Äì Product Ops Roadmap (with strong sync + multi-framework scoring)

Here‚Äôs a refined step-by-step plan reflecting everything we‚Äôve aligned on:

---

### **Phase 3.A ‚Äì Product Ops Workbook Plumbing**

**Goal:** Introduce the Product Ops workbook and basic wiring.

1. **Create Product Ops Google Sheet**

   * Spreadsheet: `Product Ops Workbook`
   * Tabs:

     * `Scoring_Inputs` (for now)
     * `Config` (for config-driven knobs, later)
     * `Simulations` (future)

2. **Share with service account**

   * Give Editor access to the same service account as other sheets.

3. **Add PRODUCT_OPS to config**

   * In JSON config (or a new one like `product_ops_config.json`):

     ```json
     {
       "product_ops": {
         "spreadsheet_id": "PRODUCT_OPS_SHEET_ID",
         "tabs": {
           "scoring_inputs": "Scoring_Inputs",
           "config": "Config"
         }
       }
     }
     ```

   * In `config.py`:

     ```python
     class ProductOpsConfig(BaseModel):
         spreadsheet_id: str
         scoring_inputs_tab: str = "Scoring_Inputs"
         config_tab: str = "Config"

     class Settings(BaseSettings):
         ...
         PRODUCT_OPS: Optional[ProductOpsConfig] = None
     ```

   * Extend the `model_validator` to load `PRODUCT_OPS` from that JSON, similar to `INTAKE_SHEETS`.

4. **Define v1 schema for `Scoring_Inputs` tab**

   Header row (simple starting point):

   | Initiative Key | RICE: Reach | RICE: Impact | RICE: Confidence | RICE: Effort | WSJF: Business Value | WSJF: Time Criticality | WSJF: Risk Reduction | WSJF: Job Size | RICE: Overall Score | WSJF: Overall Score | Comment |
   | -------------- | ----------- | ------------ | ---------------- | ------------ | -------------------- | ---------------------- | -------------------- | -------------- | ------------------- | ------------------- | ------- |

   * Namespacing rule:
     `"<FRAMEWORK>: <ParamName>"` for inputs and outputs.

   * Later we‚Äôll extend with `MATH1: paramX`, `MATH1: Overall Score`, etc.

5. **Implement `ScoringInputsReader`**

   New file: `app/sheets/scoring_inputs_reader.py`

   Responsibilities:

   * Given `spreadsheet_id` + `tab_name`:

     * Read header row.

     * Parse each header:

       * If it matches `"<FW>: <Param>"`:

         * Extract `framework` (`RICE`, `WSJF`, etc.) and normalized `param_name`.

     * Read data rows.

     * For each row, build:

       ```python
       {
           "initiative_key": "INIT-000001",
           "inputs_by_framework": {
               "RICE": {"reach": ..., "impact": ..., "confidence": ..., "effort": ...},
               "WSJF": {"business_value": ..., "time_criticality": ..., "risk_reduction": ..., "job_size": ...},
           },
           "row_index": N,
       }
       ```

     * Output: list of such row dicts.

6. **Add Flow 3 job**

   New file: `app/jobs/flow3_product_ops_job.py`

   For now:

   ```python
   def run_flow3_scoring_inputs_preview(db: Session) -> None:
       if not settings.PRODUCT_OPS:
           raise ValueError("PRODUCT_OPS not configured")

       sheet_id = settings.PRODUCT_OPS.spreadsheet_id
       tab_name = settings.PRODUCT_OPS.scoring_inputs_tab

       rows = read_scoring_inputs(sheet_id, tab_name)
       # For v1, just log how many, and maybe some examples
   ```

   This gives us a runnable first step to confirm connectivity and parsing.

---

### **Phase 3.B ‚Äì Strong Sync of Scoring Inputs (sheet ‚Üí Initiative fields)**

**Goal:** Sheet becomes the master for framework inputs; Initiative fields mirror those inputs.

1. **Define the mapping: Scoring_Inputs columns ‚Üí Initiative fields**

   Example (we can refine later, but define it explicitly):

   * `RICE: Reach` ‚Üí `Initiative.rice_reach` (or reuse some existing generic field)

   * `RICE: Impact` ‚Üí `Initiative.rice_impact`

   * `RICE: Confidence` ‚Üí `Initiative.rice_confidence`

   * `RICE: Effort` ‚Üí `Initiative.effort_engineering_days` or `Initiative.rice_effort`

   * `WSJF: Business Value` ‚Üí `Initiative.wsjf_business_value`

   * `WSJF: Time Criticality` ‚Üí `Initiative.time_sensitivity_score`

   * `WSJF: Risk Reduction` ‚Üí `Initiative.wsjf_risk_reduction`

   * `WSJF: Job Size` ‚Üí `Initiative.effort_engineering_days` or `Initiative.wsjf_job_size`

   If you don‚Äôt have those dedicated fields yet, we can:

   * Either add explicit fields (clean)
   * Or use some existing ones temporarily and document that mapping.

2. **Implement strong sync logic**

   In `flow3_product_ops_job.py`, implement:

   ```python
   def run_flow3_sync_inputs_to_initiatives(db: Session) -> None:
       # 1. Read rows from Scoring_Inputs (using ScoringInputsReader)
       # 2. For each row:
       #    - Find Initiative by initiative_key
       #    - For each (framework, param) input:
       #         - If cell is non-empty:
       #             initiative.<mapped_field> = parsed_value
       #         - If cell is empty:
       #             initiative.<mapped_field> = None
       # 3. Commit in batches
   ```

   This enforces:

   * Non-empty in sheet ‚Üí explicit value in Initiative.
   * Empty in sheet ‚Üí `None` in Initiative for that input.

   That‚Äôs your **strong sync** rule.

3. **ScoringService continues to work from Initiative fields**

   Now, when Flow 2 or Flow 3 builds `ScoreInputs` from an `Initiative`:

   * It uses these synced fields as the source of truth.
   * No hidden magic from older data.

---

### **Phase 3.C ‚Äì Multi-framework scoring from Product Ops sheet**

**Goal:** Use Scoring_Inputs as the front-end for RICE and WSJF (and later Math) scoring, for multiple frameworks per initiative at once.

1. **Extend ScoringService or add helper to accept explicit `ScoreInputs`**

   * The `score_initiative_all_frameworks` method allows scoring an initiative using all available frameworks without changing the active scoring framework.
   * The `score_all_frameworks` method processes all initiatives in the database, computing scores for both RICE and WSJF frameworks.

2. **Implement Flow 3 scoring job from Product Ops**

   In `flow3_product_ops_job.py`:

   ```python
   def run_flow3_scoring_from_sheet(db: Session) -> None:
       # Precondition: strong sync already done (or we do both in this job)
       # 1. Read scoring inputs (to know which frameworks have inputs)
       # 2. For each row:
       #    - Get Initiative
       #    - For each framework that has any non-empty inputs:
       #         - Call ScoringService.score_initiative(initiative, framework)
       # 3. Commit
   ```

   Key behavior:

   - Per initiative, you may run both RICE and WSJF if inputs exist.
   - Multiple `InitiativeScore` rows are created (one per framework).
   - On the Product Ops sheet:
     - You can write the per-framework outputs into `RICE: Overall Score`, `WSJF: Overall Score`, etc.

3. **Write framework-specific outputs back to Scoring_Inputs tab**

   * Use the same header parsing as inputs (`"RICE: Overall Score"`, `"WSJF: Overall Score"`).
   * After scoring, build a row of output values and call `update_values` to write those cells for that row only.

   So each row on `Scoring_Inputs` shows all frameworks‚Äô inputs and all frameworks‚Äô scores for that initiative.

---

### **Phase 3.D ‚Äì Config tab as control surface**

**Goal:** Move certain product-owned config knobs out of `config.py` into the Product Ops `Config` tab.

1. **Define v1 `Config` tab schema**

   Headers:

   | Key                        | Value | Type   | Scope   | Description |
   | -------------------------- | ----- | ------ | ------- | ----------- |
   | SCORING_DEFAULT_FRAMEWORK  | RICE  | string | scoring | ...         |
   | SCORING_BATCH_COMMIT_EVERY | 200   | int    | scoring | ...         |

2. **Implement `ConfigReader`**

   * `app/sheets/config_reader.py`:

     * Reads all rows where `Key` is non-empty.
     * Returns `dict[str, str]` mapping keys to raw values.

3. **Add `RuntimeConfig` or helper for overrides**

   At job start:

   * Read config sheet ‚Üí `overrides`.
   * For any relevant key (`SCORING_DEFAULT_FRAMEWORK`, `SCORING_BATCH_COMMIT_EVERY`):

     * Use override if present, else fall back to `settings`.

4. **Use sheet-driven config in Flow 2 + Flow 3**

   * In scoring jobs:

     * Use `runtime_config.scoring_default_framework` (sheet override) instead of static `settings.SCORING_DEFAULT_FRAMEWORK`.
     * Same for `SCORING_BATCH_COMMIT_EVERY`.

---

### **Phase 3.E ‚Äì Later: Simulations & Optimization tabs**

Once 3.A‚ÄìC are stable:

* Add tabs like `Simulations`, `Capacity_Scenarios`.
* Define schemas for scenarios (constraints, budgets, segments).
* Implement readers & jobs that:

  * Run Monte Carlo / LP / multi-objective schedulers.
  * Write outputs back into result columns / result tabs.

Same pattern as before:

> Tab = configuration / scenario
> Backend = engine
> Sheet = place where PMs see & tweak the outcome

---

## üß© Flow 3 Implementation Status

### **Phase 3.A ‚Äì Product Ops Workbook Plumbing** ‚úÖ COMPLETE

**Goal:** Introduce the Product Ops workbook and basic wiring.

**Implemented:**
1. ‚úÖ **Product Ops Google Sheet created**
   - Spreadsheet ID: `1zfxk-qQram2stUWYytiXapOeVh3yNulb32QYVJrOGt8`
   - Tab: `Scoring_Inputs`

2. ‚úÖ **Service account access configured**
   - Sheet shared with service account
   - Read/write permissions verified

3. ‚úÖ **Configuration in place**
   - `product_ops_config.json` created with:
     ```json
     {
       "spreadsheet_id": "1zfxk-qQram2stUWYytiXapOeVh3yNulb32QYVJrOGt8",
       "scoring_inputs_tab": "Scoring_Inputs",
       "config_tab": "Config"
     }
     ```
   - Environment variable: `PRODUCT_OPS_CONFIG_FILE=product_ops_config.json`

4. ‚úÖ **`Scoring_Inputs` schema implemented**
   - Headers support both formats:
     - Namespaced: `RICE: Reach`, `WSJF: Business Value`
     - Underscore: `rice_reach`, `wsjf_business_value`
   - Input columns (PM-editable):
     - `initiative_key`
     - `active_scoring_framework`
     - `use_math_model`
     - `rice_reach`, `rice_impact`, `rice_confidence`, `rice_effort`
     - `wsjf_business_value`, `wsjf_time_criticality`, `wsjf_risk_reduction`, `wsjf_job_size`
     - `strategic_priority_coefficient`
   - Output columns (system-populated):
     - `rice_value_score`, `rice_effort_score`, `rice_overall_score`
     - `wsjf_value_score`, `wsjf_effort_score`, `wsjf_overall_score`

5. ‚úÖ **`ScoringInputsReader` implemented**
   - File: `app/sheets/scoring_inputs_reader.py`
   - Features:
     - Flexible header parsing (handles both `RICE: Reach` and `rice_reach` formats)
     - Per-row parsing into framework-specific inputs
     - Returns structured data: `{initiative_key, active_framework, rice_{params}, wsjf_{params}}`

6. ‚úÖ **Flow 3 job implemented**
   - File: `app/jobs/flow3_product_ops_job.py`
   - Commands:
     - `--preview`: Validate sheet inputs
     - `--sync`: Strong sync sheet ‚Üí DB
     - `--compute-all`: Compute RICE & WSJF for all initiatives
     - `--write-scores`: Write per-framework scores back to sheet

7. ‚úÖ **CLI implemented**
   - File: `test_scripts/flow3_product_ops_cli.py`
   - Custom logging formatter for detailed per-row tracking

---

### **Phase 3.B ‚Äì Strong Sync of Scoring Inputs** ‚úÖ COMPLETE

**Goal:** Sheet becomes the master for framework inputs; Initiative fields mirror those inputs.

**Implemented:**
1. ‚úÖ **Field mapping defined and implemented**
   - Sheet column ‚Üí DB field mapping:
     - `rice_reach` ‚Üí `Initiative.rice_reach`
     - `rice_impact` ‚Üí `Initiative.rice_impact`
     - `rice_confidence` ‚Üí `Initiative.rice_confidence`
     - `rice_effort` ‚Üí `Initiative.rice_effort`
     - `wsjf_business_value` ‚Üí `Initiative.wsjf_business_value`
     - `wsjf_time_criticality` ‚Üí `Initiative.wsjf_time_criticality`
     - `wsjf_risk_reduction` ‚Üí `Initiative.wsjf_risk_reduction`
     - `wsjf_job_size` ‚Üí `Initiative.wsjf_job_size`
     - `active_scoring_framework` ‚Üí `Initiative.active_scoring_framework`
     - `strategic_priority_coefficient` ‚Üí `Initiative.strategic_priority_coefficient`

2. ‚úÖ **Strong sync logic implemented**
   - Function: `run_flow3_sync_inputs_to_initiatives(db, commit_every, spreadsheet_id, tab_name)`
   - Behavior:
     - Non-empty cell in sheet ‚Üí explicit value in Initiative
     - Empty cell in sheet ‚Üí `None` in Initiative (clears old values)
     - Sets `updated_source = 'flow3.productopssheet_read_inputs'` for audit trail (per `app/utils/provenance.py`)
   - Batch commit support for performance

3. ‚úÖ **Tested and validated**
   - Successfully synced 10 initiatives from Product Ops sheet
   - Verified field values match sheet inputs exactly
   - Confirmed empty cells clear DB values (strong sync enforcement)

---

### **Phase 3.C ‚Äì Multi-Framework Scoring** ‚úÖ COMPLETE

**Goal:** Compute and store RICE and WSJF scores side-by-side for comparison.

**Implemented:**
1. ‚úÖ **Per-framework score storage in DB**
   - Migration: `20251204_per_fw_scores.py`
   - New DB columns on `Initiative`:
     - `rice_value_score`, `rice_effort_score`, `rice_overall_score`
     - `wsjf_value_score`, `wsjf_effort_score`, `wsjf_overall_score`

2. ‚úÖ **ScoringService enhancements**
   - `_compute_framework_scores_only(initiative, framework, enable_history)`:
     - Computes scores for a specific framework
     - Updates **only** per-framework score fields
     - **Does NOT** change `active_scoring_framework` or active score fields
     - Prevents side effects during multi-framework scoring
   
   - `score_initiative_all_frameworks(initiative, enable_history)`:
     - Scores one initiative with **all** frameworks (RICE + WSJF)
     - Stores results in per-framework fields
     - Used by Flow 3 for side-by-side comparison
   
   - `score_all_frameworks(commit_every)`:
     - Batch version: processes all initiatives
     - Computes both RICE and WSJF for every initiative
     - Efficient batch commits

3. ‚úÖ **Product Ops writer implemented**
   - File: `app/sheets/productops_writer.py`
   - Function: `write_scores_to_productops_sheet(db, client, spreadsheet_id, tab_name)`
   - Features:
     - Reads all initiatives from DB
     - Maps score columns in sheet header
     - Builds batch update payload for all score cells
     - Uses `SheetsClient.batch_update_values()` for efficient single API call
     - Updates columns: `rice_value_score`, `rice_effort_score`, `rice_overall_score`, `wsjf_value_score`, `wsjf_effort_score`, `wsjf_overall_score`

4. ‚úÖ **Tested and validated**
   - Successfully computed RICE and WSJF scores for 10+ initiatives
   - Verified per-framework scores stored correctly in DB
   - Confirmed scores written back to Product Ops sheet
   - Verified active scoring framework remains unchanged during compute-all

---

### **Phase 3.D ‚Äì Config Tab** ‚è≥ PLANNED (Not Implemented)

**Goal:** Move product-owned config knobs from `config.py` to Product Ops `Config` tab.

**Planned features:**
1. ‚è≥ **Config tab schema**
   - Headers: `Key`, `Value`, `Type`, `Scope`, `Description`
   - Example configs:
     - `SCORING_DEFAULT_FRAMEWORK`: Default framework when none set
     - `SCORING_BATCH_COMMIT_EVERY`: Batch commit size
     - `USE_AI_SUGGESTED_SCORES`: Enable/disable AI scoring

2. ‚è≥ **ConfigReader implementation**
   - Read `Config` tab into `dict[str, str]`
   - Merge with code-based defaults

3. ‚è≥ **Runtime config service**
   - Priority: CLI override > Sheet config > Code defaults
   - Used by Flow 2 and Flow 3 jobs

**Why not implemented yet:**
- Phase 3.A-C provide core functionality
- Config extraction should be driven by actual usage pain points
- Better to consolidate and test 3.A-C first

**Next steps for 3.D:**
- Document which configs PMs actually need to change
- Implement `ConfigReader`
- Add runtime config merging logic
- Test with Flow 2 and Flow 3

---

### **Phase 3.E ‚Äì Simulations & Optimization** üìã FUTURE

**Goal:** Enable scenario planning and optimization.

**Planned features:**
- Monte Carlo simulations for risk/uncertainty
- Capacity planning and allocation
- Multi-objective optimization (value vs. cost vs. risk)
- Scenario tabs for "what-if" analysis

**Dependencies:**
- Stable Phase 3.A-C pipeline
- Production usage and feedback
- Clear PM requirements for simulation/optimization needs

---

## üéØ Current System State (As of 2025-12-09)

### ‚úÖ What Works End-to-End

**Full Flow 3 Pipeline:**
```bash
# 1. Preview inputs from Product Ops sheet
uv run python -m test_scripts.flow3_product_ops_cli --preview

# 2. Sync inputs from sheet to DB (strong sync)
uv run python -m test_scripts.flow3_product_ops_cli --sync

# 3. Compute RICE and WSJF scores for all initiatives
uv run python -m test_scripts.flow3_product_ops_cli --compute-all

# 4. Write per-framework scores back to Product Ops sheet
uv run python -m test_scripts.flow3_product_ops_cli --write-scores
```

**Integration with Flow 1 (Backlog Sync):**
- Active scores from DB ‚Üí Central Backlog sheet
- PMs can switch `active_scoring_framework` on Central Backlog
- Backlog displays the active framework's scores

**Integration with Flow 2 (Active Scoring):**
- Respects `active_scoring_framework` from Product Ops sheet
- Updates active score fields (`value_score`, `effort_score`, `overall_score`)
- Does NOT interfere with per-framework scores

### üìä Data Flow Architecture

```
Product Ops Sheet (Scoring_Inputs)
         ‚Üì
    [Flow 3 Sync]
         ‚Üì
    Initiative DB fields (rice_*, wsjf_*, active_scoring_framework)
         ‚Üì
    [Flow 3 Compute All] ‚Üí Per-framework scores (rice_*_score, wsjf_*_score)
         ‚Üì                          ‚Üì
    [Flow 3 Write]          [Flow 2 Active Scoring] ‚Üí Active scores (value_score, overall_score)
         ‚Üì                          ‚Üì
Product Ops Sheet (outputs)    Central Backlog Sheet (active scores only)
```

### üîß Key Design Decisions

1. **Strong Sync Enforcement**
   - Product Ops sheet is source of truth for inputs
   - Empty cells in sheet ‚Üí `None` in DB (clears stale data)
   - No silent fallbacks or hidden defaults

2. **Per-Framework Score Isolation**
   - `score_all_frameworks()` only touches per-framework fields
   - Active scoring framework selection remains independent
   - PMs can compare RICE vs. WSJF without losing data

3. **Separation of Concerns**
   - Flow 3: Product Ops experimentation (all frameworks)
   - Flow 2: Active scoring decision (one framework)
   - Flow 1: Backlog presentation (active framework only)

4. **Efficient Batch Operations**
   - Single API call for writing all scores (`batch_update_values`)
   - Batch commits every N initiatives (configurable)
   - Targeted cell updates (no full sheet regeneration)

---

## üìù Next Steps

### Immediate (Recommended)
1. ‚úÖ **Test Flow 1 integration** (verify Central Backlog sync)
2. ‚úÖ **Test framework switching** (change active framework, verify scores update)
3. ‚è≥ **Document PM user guide** (how to use Product Ops sheet)
4. ‚è≥ **Add integration tests** (full Flow 3 pipeline)

### Short-term (When Needed)
1. ‚è≥ **Implement Phase 3.D** (Config tab) if config changes become frequent
2. ‚è≥ **Add validation rules** (e.g., "Reach must be > 0")
3. ‚è≥ **Add error recovery** (retry logic, partial failures)

### Long-term (Future)
1. üìã **Phase 3.E** (Simulations & Optimization)
2. üìã **Math Model scoring framework** (custom formulas)
3. üìã **AI-assisted parameter suggestions**

---

## üêõ Known Issues / Limitations

1. **`---` separator rows in Product Ops sheet**
   - Warning logged but harmless
   - Skipped during sync (no DB entry exists)
   - Consider: Add validation to reject invalid initiative keys

2. **No conflict resolution for concurrent edits**
   - Last write wins (sheet ‚Üí DB)
   - Consider: Add timestamp/version tracking

3. **No rollback mechanism**
   - Strong sync is irreversible without manual DB restore
   - Consider: Add "preview mode" that shows diff before committing

4. **No input validation**
   - Accepts any numeric values (could be negative, zero, etc.)
   - Consider: Add validation rules per parameter

---
The Complete Flow:

1. Flow 3 --sync          ‚Üí Update initiative inputs in DB
2. Flow 3 --compute-all   ‚Üí Compute RICE + WSJF scores (per-framework fields)
3. Flow 3 --write-scores  ‚Üí Write per-framework scores to Product Ops
4. Flow 2 --all           ‚Üí Update ACTIVE scores from per-framework scores ‚¨ÖÔ∏è THIS WAS MISSING
5. Flow 1 (backlog sync)  ‚Üí Write active scores to Central Backlog

---

## Authoritative Propagation Path (Concise)

1) Product Ops changes ‚Üí Flow 3 `--sync` (Sheet ‚Üí DB)
2) Flow 3 `--compute-all` (per-framework only: `rice_value_score`, `rice_overall_score`, `wsjf_value_score`, `wsjf_overall_score`)
3) Flow 3 `--write-scores` (DB ‚Üí Product Ops sheet, per-framework columns)
4) Flow 2 `--all` (Activate: copy per-framework based on `active_scoring_framework` into `value_score`, `overall_score`, and `effort_score` when applicable)
5) Flow 1 Backlog Sync (DB ‚Üí Central Backlog)

Notes

- Flow 3 does NOT update `value_score`/`overall_score`; Flow 2 is REQUIRED to activate RICE/WSJF into active fields.
- Header normalization supports underscore variants (e.g., `active_scoring_framework`).

## Framework Switching: Central Backlog Test Case

Scenario: Change `active_scoring_framework` on Central Backlog (e.g., WSJF ‚Üí RICE).

Steps:
- Backlog Update (Sheet ‚Üí DB): run `run_backlog_update` (or CLI) to pull the change into DB.
- Flow 2 `--all`: update `value_score`/`overall_score` from the chosen per-framework fields.
- Backlog Sync: push updated active scores back to Central Backlog (DB ‚Üí Sheet).