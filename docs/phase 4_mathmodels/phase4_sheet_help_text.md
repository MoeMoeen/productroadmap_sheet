# Phase 4: ProductOps Sheet Help Text

**Purpose**: Sample help text for ProductOps tabs to guide PM usage  
**Target Audience**: Product Managers  
**Implementation**: Add as Row 2 (below headers) or as comment bubbles on column headers  
**Last Updated**: 16 December 2025

---

## MathModels Tab

### Row 2 Help Text (Below Headers)

| initiative_key | model_name | model_description_free_text | formula_text | approved_by_user | llm_suggested_formula_text | llm_notes | assumptions_text | model_prompt_to_llm | suggested_by_llm |
|---|---|---|---|---|---|---|---|---|---|
| **REQUIRED** Your initiative ID | Auto-generated by system | Describe your model in plain English | Write Python formula: `value = x * y` | Set TRUE after review | LLM suggestion (optional) | LLM reasoning | Document assumptions | Optional: guide LLM | System flag (auto) |

### Column Comment Bubbles (Hover Help)

**initiative_key**:
> Your initiative's unique identifier (e.g., INIT-001). Must match Scoring_Inputs tab.

**model_description_free_text**:
> Plain English description of what the model calculates. Example: "Revenue impact from new feature based on user sessions and conversion rate"

**formula_text**:
> Python formula defining value and optionally effort. Must assign `value = ...` Required syntax: `value = sessions * conv_rate * 100` Optional: `effort = eng_days` Use identifiers that will become parameters.

**approved_by_user**:
> Set to TRUE after reviewing and approving the formula. System will only score approved formulas. Set to FALSE to disable scoring.

**llm_suggested_formula_text**:
> (Read-only) LLM-generated formula suggestion. Review and copy to `formula_text` if acceptable, or write your own.

**llm_notes**:
> (Read-only) LLM's rationale for the suggested formula. Helps understand the model logic.

**assumptions_text**:
> Document any assumptions behind your model (e.g., "Assumes 5% conversion rate based on Q3 data").

**model_prompt_to_llm**:
> Optional: Provide specific instructions to guide LLM suggestion (e.g., "Focus on revenue metrics, ignore cost").

**suggested_by_llm**:
> (System field) TRUE if this formula was suggested by LLM. Auto-populated.

**model_name**:
> (System field) Auto-generated identifier. Do not edit.

---

## Params Tab

### Row 2 Help Text (Below Headers)

| initiative_key | framework | param_name | value | approved | notes | is_auto_seeded | param_display | description | unit | min | max | source |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **REQUIRED** Your initiative ID | MATH_MODEL (auto) | Parameter from formula | **FILL THIS** Enter number | **FILL THIS** Set TRUE | Your notes | System flag | Friendly name | What this means | Unit type | Min value | Max value | Where to find it |

### Column Comment Bubbles (Hover Help)

**initiative_key**:
> Your initiative's unique identifier. Matches MathModels tab.

**framework**:
> (System field) Always MATH_MODEL for custom formulas. Do not edit.

**param_name**:
> (System field) Parameter identifier extracted from formula (e.g., `sessions`, `conv_rate`). Do not edit.

**value**:
> ⭐ FILL THIS: Enter the numeric value for this parameter. Example: sessions = 10000, conv_rate = 0.05

**approved**:
> ⭐ FILL THIS: Set to TRUE after verifying the value is correct. System only uses approved params.

**notes**:
> Optional: Add your notes about this parameter (e.g., "Based on Q4 analytics report").

**is_auto_seeded**:
> (System field) TRUE if row was auto-created by param seeding. Do not edit.

**param_display**:
> (Read-only) Friendly display name suggested by LLM (e.g., "User Sessions").

**description**:
> (Read-only) Explanation of what this parameter represents, provided by LLM.

**unit**:
> (Read-only) Unit type suggested by LLM (e.g., "users", "percentage", "dollars").

**min**:
> (Read-only) Suggested minimum value for validation (LLM-provided).

**max**:
> (Read-only) Suggested maximum value for validation (LLM-provided).

**source**:
> (Read-only) Where to find this data, suggested by LLM (e.g., "Google Analytics", "Sales CRM").

---

## Scoring_Inputs Tab

### Row 2 Help Text (Below Headers)

| initiative_key | active_scoring_framework | use_math_model | rice_reach | rice_impact | rice_confidence | rice_effort | wsjf_business_value | wsjf_time_criticality | wsjf_risk_reduction | wsjf_job_size | rice_value_score | rice_overall_score | wsjf_value_score | wsjf_overall_score | math_value_score | math_overall_score | math_warnings |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| **REQUIRED** Initiative ID | **CHOOSE** RICE/WSJF/MATH_MODEL | Enable math? | RICE input | RICE input | RICE input | RICE input | WSJF input | WSJF input | WSJF input | WSJF input | Computed RICE | Computed RICE | Computed WSJF | Computed WSJF | Computed MATH | Computed MATH | Errors/warnings |

### Column Comment Bubbles (Hover Help)

**initiative_key**:
> Your initiative's unique identifier. Used to match across tabs.

**active_scoring_framework**:
> ⭐ CHOOSE: Set to RICE, WSJF, or MATH_MODEL. This determines which score is used in Central Backlog. Compare the `*_overall_score` columns to decide.

**use_math_model**:
> Set TRUE if you want to enable custom math model scoring for this initiative. Requires formula in MathModels tab.

**rice_reach / rice_impact / rice_confidence / rice_effort**:
> RICE framework inputs. Fill these to compute RICE scores. Effort is in engineering days.

**wsjf_business_value / wsjf_time_criticality / wsjf_risk_reduction / wsjf_job_size**:
> WSJF framework inputs. Fill these to compute WSJF scores. Job size is in engineering days.

**rice_value_score / rice_overall_score**:
> (Read-only) Computed RICE scores. Overall = (reach × impact × confidence) / effort

**wsjf_value_score / wsjf_overall_score**:
> (Read-only) Computed WSJF scores. Overall = cost_of_delay / job_size

**math_value_score / math_overall_score**:
> (Read-only) Computed math model scores from your custom formula. Overall = value / effort (if effort defined).

**math_warnings**:
> (Read-only) Error messages if math model failed. Check this column if `math_overall_score` is blank. Common: "Missing params: x, y" or "Formula not approved".

---

## Formatting Recommendations

### Color Coding (Suggested Background Colors)

**System Columns** (Warning-Protected, Read-Only):
- Background: Light Blue (#CFE2F3)
- Applies to: `initiative_key`, `framework`, `param_name`, `is_auto_seeded`, `model_name`, `suggested_by_llm`, etc.

**PM-Editable Columns**:
- Background: Light Green (#D9EAD3)
- Applies to: `value`, `approved`, `approved_by_user`, `formula_text`, `active_scoring_framework`

**Computed Output Columns**:
- Background: Light Yellow (#FFF2CC)
- Applies to: `*_value_score`, `*_overall_score`, `math_warnings`

**Optional/Notes Columns**:
- Background: White or Light Gray (#F3F3F3)
- Applies to: `notes`, `assumptions_text`, `model_prompt_to_llm`

### Frozen Rows/Columns

**Recommended Settings**:
- Freeze Row 1 (headers)
- Freeze Row 2 (help text) - optional but helpful
- Freeze Column A (`initiative_key`) for easy scrolling

### Font Formatting for Row 2 Help Text

- Font: Arial 9pt
- Style: Italic
- Color: Gray (#666666)
- Alignment: Center

---

## Implementation Steps

### Step 1: Add Help Text Row
1. Open ProductOps spreadsheet
2. For each tab (MathModels, Params, Scoring_Inputs):
   - Insert row below headers (Row 2)
   - Copy help text from tables above
   - Apply formatting (italic, gray, centered)

### Step 2: Add Color Coding
1. Select system columns (blue background)
2. Select editable columns (green background)
3. Select output columns (yellow background)

### Step 3: Freeze Rows/Columns
1. Format → Freeze → 2 rows
2. Format → Freeze → 1 column

### Step 4: Add Comment Bubbles (Optional)
1. Right-click column header
2. Insert comment
3. Copy text from "Column Comment Bubbles" sections above

### Step 5: Apply Warning Protections
Run the protection command (see CLI cheatsheet):
```bash
uv run python -m test_scripts.flow4_mathmodels_cli --protect-sheets
```

---

## Sample Screenshot Layout (Text Representation)

### MathModels Tab
```
Row 1 (Header):     | initiative_key | formula_text | approved_by_user | ...
                    |---[Blue Bg]----|[Green Bg]----|----[Green Bg]----| ...
Row 2 (Help Text):  | REQUIRED ID    | Python expr  | TRUE after review| ...
                    | [Gray Italic]  | [Gray Italic]| [Gray Italic]    | ...
Row 3 (Data):       | INIT-001       | value=x*y    | TRUE             | ...
```

### Params Tab
```
Row 1 (Header):     | initiative_key | param_name | value    | approved | ...
                    |---[Blue Bg]----|[Blue Bg]---|[Grn Bg]-|[Grn Bg]--| ...
Row 2 (Help Text):  | REQUIRED ID    | From formula| FILL THIS| Set TRUE| ...
                    | [Gray Italic]  | [Gray Italic]|[Gray It]|[Gray It] | ...
Row 3 (Data):       | INIT-001       | sessions   | 10000    | TRUE     | ...
```

### Scoring_Inputs Tab
```
Row 1 (Header):     | initiative_key | active_scoring_framework | math_value_score | math_warnings |
                    |---[Blue Bg]----|-------[Green Bg]---------|[Yellow Bg]-------|[Yellow Bg]----|
Row 2 (Help Text):  | REQUIRED ID    | CHOOSE: RICE/WSJF/MATH   | Computed         | Errors        |
                    | [Gray Italic]  | [Gray Italic]            | [Gray Italic]    | [Gray Italic] |
Row 3 (Data):       | INIT-001       | MATH_MODEL               | 50000            |               |
```

---

## Related Documentation
- [Phase 4 Runbook](./phase4_mathmodels_runbook.md)
- [Phase 4 Acceptance Test](./phase4_acceptance_test.md)
- [CLI Commands Cheatsheet](./cli_commands_cheatsheet.md)
